@article{Anderes2011,
   abstract = {We develop a weighted local likelihood estimate for the parameters that govern the local spatial dependency of a locally stationary random field. The advantage of this local likelihood estimate is that it smoothly downweights the influence of faraway observations, works for irregular sampling locations, and when designed appropriately, can trade bias and variance for reducing estimation error. This paper starts with an exposition of our technique on the problem of estimating an unknown positive function when multiplied by a stationary random field. This example gives concrete evidence of the benefits of our local likelihood as compared to unweighted local likelihoods. We then discuss the difficult problem of estimating a bandwidth parameter that controls the amount of influence from distant observations. Finally we present a simulation experiment for estimating the local smoothness of a local Matérn random field when observing the field at random sampling locations in [0,1]2. The local Matérn is a fully nonstationary random field, has a closed form covariance, can attain any degree of differentiability or Hölder smoothness and behaves locally like a stationary Matérn. We include an appendix that proves the positive definiteness of this covariance function. © 2010 Elsevier Inc.},
   author = {Ethan B. Anderes and Michael L. Stein},
   doi = {10.1016/j.jmva.2010.10.010},
   issn = {0047259X},
   issue = {3},
   journal = {Journal of Multivariate Analysis},
   keywords = {Local likelihood,Local parameters,Nonstationarity,Random fields},
   month = {3},
   pages = {506-520},
   title = {Local likelihood estimation for nonstationary random fields},
   volume = {102},
   year = {2011},
}
@article{Banerjee2008,
   abstract = {We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive 1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.},
   author = {Onureena Banerjee and Laurent El Ghaoui and Aspremon@princeton Edu},
   journal = {Journal of Machine Learning Research},
   keywords = {Gaussian graphical model,binary data,convex optimization,maximum likelihood estimation,model selection},
   pages = {485-516},
   title = {Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data Alexandre d'Aspremont},
   volume = {9},
   year = {2008},
}
@book{Banerjee2014,
   author = {Sudipto Banerjee and Bradley P. Carlin and Alan E. Gelfand},
   publisher = {CRC Press},
   title = {Hierarchical Modeling and Analysis for Spatial Data},
   year = {2014},
}
@article{Bardossy2011,
   abstract = {The aim of this paper is to define a method for determining reasonable estimates of rainfall modeled by global circulation models (GCMs) coupled with regional climate models (RCMs). The paper describes and uses two new procedures designed to give confidence in the interpretation of such rainfall estimates. The first of these procedures is the use of circulation patterns (CPs) to define quantile-quantile (Q-Q) transforms between observed and RCM-estimated rainfall (the CPs were derived from sea level pressure (SLP) fields obtained from reanalysis of historical daily weather in a previous study). The Q-Q transforms are derived using two downscaling techniques during a 20 year calibration period and were validated during a 10 year period of observations. The second novel procedure is the use of a double Q-Q transform to estimate the rainfall patterns and amounts from GCM-RCM predictions of SLP and rainfall fields during a future period. This procedure is essential because we find that the CP-dependent rainfall frequency distributions on each block are unexpectedly different from the corresponding historical distributions. The daily rainfall fields compared are recorded on a 25 km grid over the Rhine basin in Germany; the observed daily data are averaged over the grid blocks, and the RCM values have been estimated over the same grid. Annual extremes, recorded on each block during the validation period, of (1) maximum daily rainfall and (2) the lowest 5% of filtered rainfall were calculated to determine the ability of RCMs to capture rainfall characteristics which are important for hydrological applications. The conclusions are that (1) RCM outputs used here are good at capturing the patterns and rankings of CP-dependent rainfall; (2) CP-dependent downscaling, coupled with the double Q-Q transform, gives good estimates of the rainfall during the validation period; (3) because the RCMs offer future CP-dependent rainfall distributions that are different from the observed distributions, it is judged that these predictions, once modified by the double Q-Q transforms, are hydrologically reasonable; and (4) the climate in the Rhine basin in the future, as modeled by the RCMs, is likely to be wetter than in the past. The results suggest that such future projections may be used with cautious confidence. Copyright 2011 by the American Geophysical Union.},
   author = {András Bárdossy and Geoffrey Pegram},
   doi = {10.1029/2010WR009689},
   issn = {00431397},
   issue = {4},
   journal = {Water Resources Research},
   title = {Downscaling precipitation using regional climate models and circulation patterns toward hydrology},
   volume = {47},
   year = {2011},
}
@article{Ben-David2011,
   abstract = {In this paper, we consider Gaussian models Markov with respect to an arbitrary DAG. We first construct a family of conjugate priors for the Cholesky parametrization of the covariance matrix of such models. This family has as many shape parameters as the DAG has vertices, and naturally extends the work of Geiger and Heckerman [8]. From these distributions, we derive prior distributions for the covariance and precision parameters of the Gaussian DAG Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and Letac and Massam [16] for Gaussian models Markov with respect to a decomposable graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart distributions. An advantage of these distributions is that they possess strong hyper Markov properties and thus allow for explicit estimation of the covariance and precision parameters, regardless of the dimension of the problem. They also allow us to develop methodology for model selection and covariance estimation in the space of DAG-Markov models. We demonstrate via several numerical examples that the proposed method scales well to high-dimensions.},
   author = {Emanuel Ben-David and Tianxi Li and Helene Massam and Bala Rajaratnam},
   journal = {arXiv preprint arXiv:1109.4371},
   month = {9},
   title = {High dimensional Bayesian inference for Gaussian directed acyclic graph models},
   url = {http://arxiv.org/abs/1109.4371},
   year = {2011},
}
@article{Castruccio2014,
   author = {Stefano Castruccio and David J Mcinerney and Michael L Stein and Feifei Liu Crouch and Robert L Jacob and Elisabeth J Moyer},
   doi = {10.1175/JCLI-D-13},
   title = {Statistical Emulation of Climate Model Projections Based on Precomputed GCM Runs*},
   journal = {Journal of Climate},
   volume = {27},
   pages = {1829-1844},
   year = {2014},
}
@article{Cressie2016,
   abstract = {Multivariate geostatistics is based on modelling all covariances between all possible combinations of two or more variables at any sets of locations in a continuously indexed domain. Multivariate spatial covariance models need to be built with care, since any covariance matrix that is derived from such a model must be nonnegative-definite. In this article, we develop a conditional approach for spatial-model construction whose validity conditions are easy to check. We start with bivariate spatial covariance models and go on to demonstrate the approach's connection to multivariate models defined by networks of spatial variables. In some circumstances, such as modelling respiratory illness conditional on air pollution, the direction of conditional dependence is clear. When it is not, the two directional models can be compared. More generally, the graph structure of the network reduces the number of possible models to compare. Model selection then amounts to finding possible causative links in the network. We demonstrate our conditional approach on surface temperature and pressure data, where the role of the two variables is seen to be asymmetric.},
   author = {Noel Cressie and Andrew Zammit-Mangion},
   doi = {10.1093/biomet/asw045},
   issn = {14643510},
   issue = {4},
   journal = {Biometrika},
   keywords = {Asymmetry,Cross-covariance function,Directed acyclic graph,Kriging,Multivariate geostatistics},
   month = {12},
   pages = {915-935},
   publisher = {Oxford University Press},
   title = {Multivariate spatial covariance models: A conditional approach},
   volume = {103},
   year = {2016},
}
@article{Cressie2022,
   author = {Noel Cressie and Michael Bertolacci and Andrew Zammit‐Mangion},
   doi = {10.1029/2022GL098277},
   issn = {0094-8276},
   issue = {14},
   journal = {Geophysical Research Letters},
   month = {7},
   title = {From Many to One: Consensus Inference in a MIP},
   volume = {49},
   url = {https://onlinelibrary.wiley.com/doi/10.1029/2022GL098277},
   year = {2022},
}
@article{Damian2001,
   abstract = {We use the Sampson and Guttorp approach to model the non-stationary correlation function r(x, x′) of a Gaussian spatial process through a bijective space deformation, f, so that in the deformed space the spatial correlation function can be considered isotropic, namely r(x, x′) = ρ(|f(x)-f(x′)|), where ρ belongs to a known parametric family. Given the locations in the deformed space of a number of geographic sites at which data are available, we smoothly extrapolate the deformation to the whole region of interest. Using a Bayesian framework, we estimate jointly these locations, as well as the parameters of the correlation function and the variance parameters. The advantage of our Bayesian approach is that it allows us to obtain measures of uncertainty of all these parameters. As the parameter space is of a very high dimension, we implement an MCMC method for obtaining samples from the posterior distributions of interest. We demonstrate our method through a simulation study, and show an application to a real data set. Copyright © 2001 John Wiley & Sons, Ltd.},
   author = {Doris Damian and Paul D. Sampson and Peter Guttorp},
   doi = {10.1002/1099-095X(200103)12:2<161::AID-ENV452>3.0.CO;2-G},
   issn = {11804009},
   issue = {2},
   journal = {Environmetrics},
   keywords = {Gaussian spatial processes,Markov Chain Monte Carlo,Thin-plate splines},
   pages = {161-178},
   title = {Bayesian estimation of semi-parametric non-stationary spatial covariance structures},
   volume = {12},
   year = {2001},
}
@article{Daniels2002,
   abstract = {S Parsimonious modelling of the within-subject covariance structure while heeding its positive-definiteness is of great importance in the analysis of longitudinal data. Using the Cholesky decomposition and the ensuing unconstrained and statistically meaningful repara-meterisation, we provide a convenient and intuitive framework for developing conditionally conjugate prior distributions for covariance matrices and show their connections with generalised inverse Wishart priors. Our priors offer many advantages with regard to elicitation, positive definiteness, computations using Gibbs sampling, shrinking covari-ances toward a particular structure with considerable flexibility, and modelling covariances using covariates. Bayesian estimation methods are developed and the results are compared using two simulation studies. These simulations suggest simpler and more suitable priors for the covariance structure of longitudinal data.},
   author = {Michael J Daniels and Mohsen Pourahmadi},
   issue = {3},
   journal = {Biometrika},
   keywords = {Bayes estimate,Hierarchical model,Markov chain Monte Carlo,Mixed model,Shrinkage estimator,Some key words: Antedependence and autoregressive models,Time series model,Unconstrained parameterisation,Wishart distribution},
   pages = {553-566},
   title = {Bayesian analysis of covariance matrices and dynamic models for longitudinal data},
   volume = {89},
   url = {https://academic.oup.com/biomet/article/89/3/553/251834},
   year = {2002},
}
@article{Datta2016a,
   abstract = {Spatial process models for analyzing geostatistical data entail computations that become prohibitive as the number of spatial locations become large. This article develops a class of highly scalable nearest-neighbor Gaussian process (NNGP) models to provide fully model-based inference for large geostatistical datasets. We establish that the NNGP is a well-defined spatial process providing legitimate finite-dimensional Gaussian densities with sparse precision matrices. We embed the NNGP as a sparsity-inducing prior within a rich hierarchical modeling framework and outline how computationally efficient Markov chain Monte Carlo (MCMC) algorithms can be executed without storing or decomposing large matrices. The floating point operations (flops) per iteration of this algorithm is linear in the number of spatial locations, thereby rendering substantial scalability. We illustrate the computational and inferential benefits of the NNGP over competing methods using simulation studies and also analyze forest biomass from a massive U.S. Forest Inventory dataset at a scale that precludes alternative dimension-reducing methods. Supplementary materials for this article are available online.},
   author = {Abhirup Datta and Sudipto Banerjee and Andrew O. Finley and Alan E. Gelfand},
   doi = {10.1080/01621459.2015.1044091},
   issn = {1537274X},
   issue = {514},
   journal = {Journal of the American Statistical Association},
   keywords = {Bayesian modeling,Gaussian process,Hierarchical models,Markov chain Monte Carlo,Nearest neighbors,Predictive process,Reduced-rank models,Sparse precision matrices,Spatial cross-covariance functions},
   month = {4},
   pages = {800-812},
   publisher = {American Statistical Association},
   title = {Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets},
   volume = {111},
   year = {2016},
}
@article{Datta2016b,
   abstract = {Particulate matter (PM) is a class of malicious environmental pollutants known to be detrimental to human health. Regulatory efforts aimed at curbing PM levels in different countries often require high resolution space–time maps that can identify red-flag regions exceeding statutory concentration limits. Continuous spatio-temporal Gaussian Process (GP) models can deliver maps depicting predicted PM levels and quantify predictive uncertainty. However, GP-based approaches are usually thwarted by computational challenges posed by large datasets. We construct a novel class of scalable Dynamic Nearest Neighbor Gaussian Process (DNNGP) models that can provide a sparse approximation to any spatio-temporal GP (e.g., with nonseparable covariance structures). The DNNGP we develop here can be used as a sparsity-inducing prior for spatio-temporal random effects in any Bayesian hierarchical model to deliver full posterior inference. Storage and memory requirements for a DNNGP model are linear in the size of the dataset, thereby delivering massive scalability without sacrificing inferential richness. Extensive numerical studies reveal that the DNNGP provides substantially superior approximations to the underlying process than low-rank approximations. Finally, we use the DNNGP to analyze a massive air quality dataset to substantially improve predictions of PM levels across Europe in conjunction with the LOTOS-EUROS chemistry transport models (CTMs).},
   author = {Abhirup Datta and Sudipto Banerjee and Andrew O. Finley and Nicholas A.S. Hamm and Martijn Schaap},
   doi = {10.1214/16-AOAS931},
   issn = {19417330},
   issue = {3},
   journal = {Annals of Applied Statistics},
   keywords = {Bayesian inference,Environmental pollutants,Markov chain Monte Carlo,Nearest neighbors,Nonseparable spatio-temporal models,Scalable Gaussian process},
   month = {9},
   pages = {1286-1316},
   publisher = {Institute of Mathematical Statistics},
   title = {Nonseparable dynamic nearest neighbor Gaussian process models for large spatio-temporal data with an application to particulate matter analysis},
   volume = {10},
   year = {2016},
}
@article{Dempster1972,
   author = {A.P. Dempster},
   issue = {1},
   journal = {Biometrics},
   pages = {157-175},
   title = {Covariance Selection},
   volume = {28},
   year = {1972},
}
@article{Dey2020,
   abstract = {For multivariate spatial Gaussian process (GP) models, customary specifications of cross-covariance functions do not exploit relational inter-variable graphs to ensure process-level conditional independence among the variables. This is undesirable, especially for highly multivariate settings, where popular cross-covariance functions such as the multivariate Mat\'ern suffer from a "curse of dimensionality" as the number of parameters and floating point operations scale up in quadratic and cubic order, respectively, in the number of variables. We propose a class of multivariate "Graphical Gaussian Processes" using a general construction called "stitching" that crafts cross-covariance functions from graphs and ensures process-level conditional independence among variables. For the Mat\'ern family of functions, stitching yields a multivariate GP whose univariate components are Mat\'ern GPs, and conforms to process-level conditional independence as specified by the graphical model. For highly multivariate settings and decomposable graphical models, stitching offers massive computational gains and parameter dimension reduction. We demonstrate the utility of the graphical Mat\'ern GP to jointly model highly multivariate spatial data using simulation examples and an application to air-pollution modelling.},
   author = {Debangan Dey and Abhirup Datta and Sudipto Banerjee},
   month = {9},
   title = {Graphical Gaussian Process Models for Highly Multivariate Spatial Data},
   url = {http://arxiv.org/abs/2009.04837},
   year = {2020},
}
@article{Fan2008,
  title={Statistical methods with varying coefficient models},
  author={Fan, Jianqing and Zhang, Wenyang},
  journal={Statistics and its Interface},
  volume={1},
  number={1},
  pages={179--195},
  year={2008},
  publisher={International Press of Boston}
}
@article{Finley2020,
   abstract = {This paper describes and illustrates new functionality for fitting spatially varying coefficients models in the spBayes (version 0.4–2) R package. The new spSVC function uses a computationally efficient Markov chain Monte Carlo algorithm and extends current spBayes functions, that fit only space-varying intercept regression models, to fit independent or multivariate Gaussian process random effects for any set of columns in the regression design matrix. Newly added OpenMP parallelization options for spSVC are discussed and illustrated, as well as helper functions for joint and point-wise prediction and model fit diagnostics. The utility of the proposed models is illustrated using a PM10 analysis over central Europe.},
   author = {Andrew O. Finley and Sudipto Banerjee},
   doi = {10.1016/j.envsoft.2019.104608},
   issn = {13648152},
   journal = {Environmental Modelling and Software},
   keywords = {Kriging,MCMC,Multivariate Gaussian process,R},
   month = {3},
   publisher = {Elsevier Ltd},
   title = {Bayesian spatially varying coefficient models in the spBayes R package},
   volume = {125},
   year = {2020},
}
@article{Friedman2008,
    abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm - the graphical lasso - that is remarkably fast: It solves a 1000-node problem (∼500000 parameters) in at most a minute and is 30-4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and Bühlmann (2006). We illustrate the method on some cell-signaling data from proteomics. © The Author 2007. Published by Oxford University Press. All rights reserved.},
    author   = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
    doi      = {10.1093/biostatistics/kxm045},
    issn     = {14654644},
    issue    = {3},
    journal  = {Biostatistics},
    keywords = {Gaussian covariance,Graphical model,L1,Lasso},
    month    = {7},
    pages    = {432-441},
    pmid     = {18079126},
    title    = {Sparse inverse covariance estimation with the graphical lasso},
    volume   = {9},
    year     = {2008}
}
@article{Fuentes2008,
   abstract = {Spectral methods are powerful tools to study and model the dependency structure of spatial temporal processes. However, standard spectral approaches as well as geostatistical methods assume separability and stationarity of the covariance function; these can be very unrealistic assumptions in many settings. In this work, we introduce a general and flexible parametric class of spatial temporal covariance models, that allows for lack of stationarity and separability by using a spectral representation of the process. This new class of covariance models has a unique parameter that indicates the strength of the interaction between the spatial and temporal components; it has the separable covariance model as a particular case. We introduce an application with ambient ozone air pollution data provided by the U.S. Environmental Protection Agency (U.S. EPA). Copyright © 2007 John Wiley & Sons, Ltd.},
   author = {Montserrat Fuentes and Li Chen and Jerry M. Davis},
   doi = {10.1002/env.891},
   issn = {11804009},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Ambient ozone,Non separable covariance,Nonseparability,Nonstationarity,Ozone modeling,Spatial covariance,Spatial temporal models,Spectral density,Spectral domain},
   month = {8},
   pages = {487-507},
   pmid = {19829763},
   title = {A class of nonseparable and nonstationary spatial temporal covariance functions},
   volume = {19},
   year = {2008},
}
@article{Fuentes2013,
   abstract = {In this paper we develop a nonparametric multivariate spatial model that avoids specifying a Gaussian distribution for spatial random effects. Our nonparametric model extends the stick-breaking (SB) prior of Sethuraman (1994), that is frequently used in Bayesian modelling to capture uncertainty in the parametric form of an outcome. The stick-breaking prior is extended here to the spatial setting by assigning each location a different, unknown distribution, and smoothing the distributions in space with a series of space-dependent kernel functions that have a space-varying bandwidth parameter. This results in a flexible nonstationary spatial model, as different kernel functions lead to different relationships between the distributions at nearby locations. This approach is the first to allow both the probabilities and the point mass values of the SB prior to depend on space. Thus, there is no need for replications and we obtain a continuous process in the limit. We extend the model to the multivariate setting by having, for each process, a different kernel function, but sharing the location of the kernel knots across the different processes. The resulting covariance for the multivariate process is in general nonstationary and nonseparable. The modelling framework proposed here is also computationally efficient because it avoids inverting large matrices and calculating determinants. We study the theoretical properties of the proposed multivariate spatial process. The methods are illustrated using simulated examples and an air pollution application to model components of fine particulate matter.},
   author = {Montserrat Fuentes and Brian Reich},
   doi = {10.5705/ss.2011.172},
   issn = {10170405},
   issue = {1},
   journal = {Statistica Sinica},
   keywords = {Dirichlet processes,Nonseparability,Nonstationarity,Spatial models},
   month = {1},
   pages = {75-97},
   title = {Multivariate spatial nonparametric modelling via kernel processes mixing},
   volume = {23},
   year = {2013},
}
@article{Geinitz2015,
   abstract = {Projections of future climate conditions are carried out by many research institutions, each with their own general circulation model to do so. The projections are additionally subjected to distinct anthropogenic forcings, specified by future greenhouse gas emissions scenarios. These two factors, together with their temporal effects and interaction, create several potential sources of variation in final climate projection output. Multilevel statistical models, and specifically multilevel ANOVA, have come to be widely used for many reasons, not least of which is their ability to comprehensively assess many different sources of variation. In this article, a Bayesian multilevel ANOVA approach is applied to climate projections to assess each of these sources of variation, estimate the uncertainty regarding the assessment, and to allow comparison across all sources. The data originate from phase three of the Coupled Model Intercomparison Project (CMIP3), consisting of 11 circulation models and three emissions scenarios over nine decadal time periods for boreal summer and winter. Data from the next phase, CMIP5, is now becoming available. As this approach towards ANOVA is relatively novel, and particularly so for spatial data, a short discussion of conventional ANOVA and the new methodology is provided.},
   author = {Steven Geinitz and Reinhard Furrer and Stephan R. Sain},
   doi = {10.1002/joc.3991},
   issn = {10970088},
   issue = {3},
   journal = {International Journal of Climatology},
   keywords = {Climate change,Uncertainty,Variance components},
   month = {3},
   pages = {433-443},
   publisher = {John Wiley and Sons Ltd},
   title = {Bayesian multilevel analysis of variance for relative comparison across sources of global climate model variability},
   volume = {35},
   year = {2015},
}
@article{Gelfand2003,
   abstract = {In many applications, the objective is to build regression models to explain a response variable over a region of interest under the assumption that the responses are spatially correlated. In nearly all of this work, the regression coefficients are assumed to be constant over the region. However, in some applications, coefficients are expected to vary at the local or subregional level. Here we focus on the local case. Although parametric modeling of the spatial surface for the coefficient is possible, here we argue that it is more natural and flexible to view the surface as a realization from a spatial process. We show how such modeling can be formalized in the context of Gaussian responses providing attractive interpretation in terms of both random effects and explaining residuals. We also offer extensions to generalized linear models and to spatio-temporal setting. We illustrate both static and dynamic modeling with a dataset that attempts to explain (log) selling price of single-family houses.},
   author = {Alan E. Gelfand and Hyon Jung Kim and C. F. Sirmans and Sudipto Banerjee},
   doi = {10.1198/016214503000170},
   issn = {01621459},
   issue = {462},
   journal = {Journal of the American Statistical Association},
   keywords = {Bayesian framework,Multivariate spatial processes,Prediction,Spatio-temporal modeling,Stationary Gaussian process},
   month = {6},
   pages = {387-396},
   title = {Spatial modeling with spatially varying coefficient processes},
   volume = {98},
   year = {2003},
}
@article{Gelfand2004,
   abstract = {Models for the analysis of multivariate spatial data are receiving increased attention these days. In many applications it will be preferable to work with multivariate spatial processes to specify such models. A critical specification in providing these models is the cross covariance function. Constructive approaches for developing valid cross-covariance functions offer the most practical strategy for doing this. These approaches include separability, kernel convolution or moving average methods, and convolution of covariance functions. We review these approaches but take as our main focus the computationally manageable class referred to as the linear model of coregionalization (LMC). We introduce a fully Bayesian development of the LMC. We offer clarification of the connection between joint and conditional approaches to fitting such models including prior specifications. However, to substantially enhance the usefulness of such modelling we propose the notion of a spatially varying LMC (SVLMC) providing a very rich class of multivariate nonstationary processes with simple interpretation. We illustrate the use of our proposed SVLMC with application to more than 600 commercial property transactions in three quite different real estate markets, Chicago, Dallas and San Diego. Bivariate nonstationary process models are developed for income from and selling price of the property.},
   author = {Alan E. Gelfand and Alexandra M. Schmidt and Sudipto Banerjee and C. F. Sirmans and Montserrat Fuentes and Dave Higdon and Bruno Sansó},
   doi = {10.1007/BF02595775},
   issn = {11330686},
   issue = {2},
   journal = {Test},
   keywords = {Cross-covariance function,Linear model of coregionalization,Matricvariate Wishart spatial process,Prior parametrization,Spatial range,Spatially varying process model},
   pages = {263-312},
   publisher = {Sociedad de Estadistica e Investigacion Operativa},
   title = {Nonstationary multivariate process modeling through spatially varying coregionalization},
   volume = {13},
   year = {2004},
}
@article{Gelfand2005a,
   abstract = {There is a considerable literature in spatiotemporal modelling. The approach adopted here applies to the setting where space is viewed as continuous but time is taken to be discrete. We view the data as a time series of spatial processes and work in the setting of dynamic models, achieving a class of dynamic models for such data. We seek rich, flexible, easy-to-specify, easy-to-interpret, computationally tractable specifications which allow very general mean structures and also non-stationary association structures. Our modelling contributions are as follows. In the case where univariate data are collected at the spatial locations, we propose the use of a spatiotemporally varying coefficient form. In the case where multivariate data are collected at the locations, we need to capture associations among measurements at a given location and time as well as dependence across space and time. We propose the use of suitable multivariate spatial process models developed through coregionalization. We adopt a Bayesian inference framework. The resulting posterior and predictive inference enables summaries in the form of tables and maps, which help to reveal the nature of the spatiotemporal behaviour as well as the associated uncertainty. We illuminate various computational issues and then apply our models to the analysis of climate data obtained from the National Center for Atmospheric Research to analyze precipitation and temperature measurements obtained in Colorado in 1997. Copyright © 2005 John Wiley & Sons, Ltd.},
   author = {Alan E. Gelfand and Sudipto Banerjee and Dani Gamerman},
   doi = {10.1002/env.715},
   issn = {11804009},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Bayesian inference,Coregionalization,Dynamic models,Multivariate spatial processes,Non-stationarity,Spatially varying coefficients},
   month = {8},
   pages = {465-479},
   title = {Spatial process modelling for univariate and multivariate dynamic spatial data},
   volume = {16},
   year = {2005},
}
@article{Gelfand2005b,
   abstract = {Customary modeling for continuous point-referenced data assumes a Gaussian process that is often taken to be stationary. When such models are fitted within a Bayesian framework, the unknown parameters of the process are assumed to be random, so a random Gaussian process results. Here we propose a novel spatial Dirichlet process mixture model to produce a random spatial process that is neither Gaussian nor stationary. We first develop a spatial Dirichlet process model for spatial data and discuss its properties. Because of familiar limitations associated with direct use of Dirichlet process models, we introduce mixing by convolving this process with a pure error process. We then examine properties of models created through such Dirichlet process mixing. In the Bayesian framework, we implement posterior inference using Gibbs sampling. Spatial prediction raises interesting questions, but these can be handled. Finally, we illustrate the approach using simulated data, as well as a dataset involving precipitation measurements over the Languedoc-Roussillon region in southern France. © 2005 American Statistical Association.},
   author = {Alan E. Gelfand and Athanasios Kottas and Steven N. Maceachern},
   doi = {10.1198/016214504000002078},
   issn = {01621459},
   issue = {471},
   journal = {Journal of the American Statistical Association},
   keywords = {Dependent Dirichlet process,Dirichlet process mixture models,Gaussian process,Markov chain Monte Carlo,Nonstationarity,Point-referenced spatial data,Random distribution},
   month = {9},
   pages = {1021-1035},
   title = {Bayesian nonparametric spatial modeling with dirichlet process mixing},
   volume = {100},
   year = {2005},
}
@article{Guinness2016,
   abstract = {Introducing flexible covariance functions is critical for interpolating spatial data since the properties of interpolated surfaces depend on the covariance function used for Kriging. An extensive literature is devoted to covariance functions on Euclidean spaces, where the Matérn covariance family is a valid and flexible parametric family capable of controlling the smoothness of corresponding stochastic processes. Many applications in environmental statistics involve data located on spheres, where less is known about properties of covariance functions, and where the Matérn is not generally a valid model with great circle distance metric. In this paper, we advance the understanding of covariance functions on spheres by defining the notion of and proving a characterization theorem for m times mean square differentiable processes on d-dimensional spheres. Stochastic processes on spheres are commonly constructed by restricting processes on Euclidean spaces to spheres of lower dimension. We prove that the resulting sphere-restricted process retains its differentiability properties, which has the important implication that the Matérn family retains its full range of smoothness when applied to spheres so long as Euclidean distance is used. The restriction operation has been questioned for using Euclidean instead of great circle distance. To address this question, we construct several new covariance functions and compare them to the Matérn with Euclidean distance on the task of interpolating smooth and non-smooth datasets. The Matérn with Euclidean distance is not outperformed by the new covariance functions or the existing covariance functions, so we recommend using the Matérn with Euclidean distance due to the ease with which it can be computed.},
   author = {Joseph Guinness and Montserrat Fuentes},
   doi = {10.1016/j.jmva.2015.08.018},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Fourier series,Kriging,Positive definite functions},
   month = {1},
   pages = {143-152},
   publisher = {Academic Press Inc.},
   title = {Isotropic covariance functions on spheres: Some properties and modeling considerations},
   volume = {143},
   year = {2016},
}
@article{Guinness2018,
   abstract = {Vecchia’s approximate likelihood for Gaussian process parameters depends on how the observations are ordered, which has been cited as a deficiency. This article takes the alternative standpoint that the ordering can be tuned to sharpen the approximations. Indeed, the first part of the article includes a systematic study of how ordering affects the accuracy of Vecchia’s approximation. We demonstrate the surprising result that random orderings can give dramatically sharper approximations than default coordinate-based orderings. Additional ordering schemes are described and analyzed numerically, including orderings capable of improving on random orderings. The second contribution of this article is a new automatic method for grouping calculations of components of the approximation. The grouping methods simultaneously improve approximation accuracy and reduce computational burden. In common settings, reordering combined with grouping reduces Kullback–Leibler divergence from the target model by more than a factor of 60 compared to ungrouped approximations with default ordering. The claims are supported by theory and numerical results with comparisons to other approximations, including tapered covariances and stochastic partial differential equations. Computational details are provided, including the use of the approximations for prediction and conditional simulation. An application to space-time satellite data is presented.},
   author = {Joseph Guinness},
   doi = {10.1080/00401706.2018.1437476},
   issn = {15372723},
   issue = {4},
   journal = {Technometrics},
   keywords = {Conditional simulation,Kriging,Parallel computation,Spatial–temporal data,Vecchia’s approximation},
   month = {10},
   pages = {415-429},
   publisher = {American Statistical Association},
   title = {Permutation and Grouping Methods for Sharpening Gaussian Process Approximations},
   volume = {60},
   year = {2018},
}
@book{HandbookEES,
   city = {Boca Raton : Taylor & Francis, 2018.},
   doi = {10.1201/9781315152509},
   editor = {Alan E. Gelfand and Montserrat Fuentes and Jennifer A. Hoeting and Richard Lyttleton Smith},
   isbn = {9781315152509},
   month = {1},
   publisher = {Chapman and Hall/CRC},
   title = {Handbook of Environmental and Ecological Statistics},
   year = {2019},
}
@book{HandbookGraphicalModels,
   city = {Boca Raton, Florida : CRC Press, c2019.},
   doi = {10.1201/9780429463976},
   editor = {Marloes Maathuis and Mathias Drton and Steffen Lauritzen and Martin Wainwright},
   isbn = {9780429463976},
   month = {11},
   publisher = {CRC Press},
   title = {Handbook of Graphical Models},
   year = {2018},
}
@book{HandbookSpatialStats,
   doi = {10.1201/9781420072884},
   editor = {Alan E. Gelfand and Peter J. Diggle and Montserrat Fuentes and Peter Guttorp},
   isbn = {9780429136504},
   month = {3},
   publisher = {CRC Press},
   title = {Handbook of Spatial Statistics},
   year = {2010},
}
@article{Heaton2019,
   abstract = {The Gaussian process is an indispensable tool for spatial data analysts. The onset of the “big data” era, however, has lead to the traditional Gaussian process being computationally infeasible for modern spatial data. As such, various alternatives to the full Gaussian process that are more amenable to handling big spatial data have been proposed. These modern methods often exploit low-rank structures and/or multi-core and multi-threaded computing environments to facilitate computation. This study provides, first, an introductory overview of several methods for analyzing large spatial data. Second, this study describes the results of a predictive competition among the described methods as implemented by different groups with strong expertise in the methodology. Specifically, each research group was provided with two training datasets (one simulated and one observed) along with a set of prediction locations. Each group then wrote their own implementation of their method to produce predictions at the given location and each was subsequently run on a common computing environment. The methods were then compared in terms of various predictive diagnostics. Supplementary materials regarding implementation details of the methods and code are available for this article online.},
   author = {Matthew J. Heaton and Abhirup Datta and Andrew O. Finley and Reinhard Furrer and Joseph Guinness and Rajarshi Guhaniyogi and Florian Gerber and Robert B. Gramacy and Dorit Hammerling and Matthias Katzfuss and Finn Lindgren and Douglas W. Nychka and Furong Sun and Andrew Zammit-Mangion},
   doi = {10.1007/s13253-018-00348-w},
   issn = {1085-7117},
   issue = {3},
   journal = {Journal of Agricultural, Biological and Environmental Statistics},
   keywords = {Big data,Gaussian process,Low-rank approximation,Parallel computing},
   month = {9},
   pages = {398-425},
   publisher = {Springer New York LLC},
   title = {A Case Study Competition Among Methods for Analyzing Large Spatial Data},
   volume = {24},
   url = {http://link.springer.com/10.1007/s13253-018-00348-w},
   year = {2019},
}
@article{Huang2006,
   abstract = {We propose a nonparametric method for identifying parsimony and for producing a statistically efficient estimator of a large covariance matrix. We reparameterise a covariance matrix through the modified Cholesky decomposition of its inverse or the one-step-ahead predictive representation of the vector of responses and reduce the nonintuitive task of modelling covariance matrices to the familiar task of model selection and estimation for a sequence of regression models. The Cholesky factor containing these regression coefficients is likely to have many off-diagonal elements that are zero or close to zero. Penalised normal likelihoods in this situation with L 1 and L 2 penalities are shown to be closely related to Tibshirani's (1996)  approach and to ridge regression. Adding either penalty to the likelihood helps to produce more stable estimators by introducing shrinkage to the elements in the Cholesky factor, while, because of its singularity, the L 1 penalty will set some elements to zero and produce interpretable models. An algorithm is developed for computing the estimator and selecting the tuning parameter. The proposed maximum penalised likelihood estimator is illustrated using simulation and a real dataset involving estimation of a 102×102 covariance matrix.},
   author = {Jianhua Z Huang and Naiping Liu and Mohsen Pourahmadi and Linxu Liu},
   issue = {1},
   journal = {Biometrika},
   keywords = {Crossvalidation,L p penalty,LASSO,Model selection,Penalised likelihood,Shrinkage,Some key words: Cholesky decomposition},
   pages = {85-98},
   title = {Covariance matrix selection and estimation via penalised normal likelihood},
   volume = {93},
   url = {https://academic.oup.com/biomet/article/93/1/85/235960},
   year = {2006},
}
@article{Huang2010,
  title={Spatial Lasso with applications to GIS model selection},
  author={Huang, Hsin-Cheng and Hsu, Nan-Jung and Theobald, David M and Breidt, F Jay},
  journal={Journal of Computational and Graphical Statistics},
  volume={19},
  number={4},
  pages={963--983},
  year={2010},
  publisher={Taylor \& Francis}
}
@article{Huang2021,
   abstract = {As spatial datasets are becoming increasingly large and unwieldy, exact inference on spatial models becomes computationally prohibitive. Various approximation methods have been proposed to reduce the computational burden. Although comprehensive reviews on these approximation methods exist, comparisons of their performances are limited to small and medium sizes of datasets for a few selected methods. To achieve a comprehensive comparison comprising as many methods as possible, we organized the Competition on Spatial Statistics for Large Datasets. This competition had the following novel features: (1) we generated synthetic datasets with the ExaGeoStat software so that the number of generated realizations ranged from 100 thousand to 1 million; (2) we systematically designed the data-generating models to represent spatial processes with a wide range of statistical properties for both Gaussian and non-Gaussian cases; (3) the competition tasks included both estimation and prediction, and the results were assessed by multiple criteria; and (4) we have made all the datasets and competition results publicly available to serve as a benchmark for other approximation methods. In this paper, we disclose all the competition details and results along with some analysis of the competition outcomes.},
   author = {Huang Huang and Sameh Abdulah and Ying Sun and Hatem Ltaief and David E. Keyes and Marc G. Genton},
   doi = {10.1007/s13253-021-00457-z},
   issn = {15372693},
   issue = {4},
   journal = {Journal of Agricultural, Biological, and Environmental Statistics},
   keywords = {Gaussian processes,Matérn covariance function,Parameter estimation,Prediction,Tukey g-and-h random fields},
   month = {12},
   pages = {580-595},
   publisher = {Springer},
   title = {Competition on Spatial Statistics for Large Datasets},
   volume = {26},
   year = {2021},
}
@article{Huser2022,
   abstract = {Max-stable processes are the most popular models for high-impact spatial extreme events, as they arise as the only possible limits of spatially-indexed block maxima. However, likelihood inference for such models suffers severely from the curse of dimensionality, since the likelihood function involves a combinatorially exploding number of terms. In this paper, we propose using the Vecchia approximation, which conveniently decomposes the full joint density into a linear number of low-dimensional conditional density terms based on well-chosen conditioning sets designed to improve and accelerate inference in high dimensions. Theoretical asymptotic relative efficiencies in the Gaussian setting and simulation experiments in the max-stable setting show significant efficiency gains and computational savings using the Vecchia likelihood approximation method compared to traditional composite likelihoods. Our application to extreme sea surface temperature data at more than a thousand sites across the entire Red Sea further demonstrates the superiority of the Vecchia likelihood approximation for fitting complex models with intractable likelihoods, delivering significantly better results than traditional composite likelihoods, and accurately capturing the extremal dependence structure at lower computational cost.},
   author = {Raphaël Huser and Michael L. Stein and Peng Zhong},
   journal = {arxiv Preprint},
   month = {3},
   title = {Vecchia Likelihood Approximation for Accurate and Fast Inference in Intractable Spatial Extremes Models},
   url = {http://arxiv.org/abs/2203.05626},
   year = {2022},
}
@article{Jun2008,
   abstract = {A limited number of complex numerical models that simulate the Earth's atmosphere, ocean, and land processes are the primary tool to study how climate may change over the next century due to anthropogenic emissions of greenhouse gases. A standard assumption is that these climate models are random samples from a distribution of possible models centered around the true climate. This implies that agreement with observations and the predictive skill of climate models will improve as more models are added to an average of the models. In this article we present a statistical methodology to quantify whether climate models are indeed unbiased and whether and where model biases are correlated across models. We consider the simulated mean state and the simulated trend over the period 1970-1999 for Northern Hemisphere summer and winter temperature. The key to the statistical analysis is a spatial model for the bias of each climate model and the use of kernel smoothing to estimate the correlations of biases across different climate models. The spatial model is particularly important to determine statistical significance of the estimated correlations under the hypothesis of independent climate models. Our results suggest that most of the climate model bias patterns are indeed correlated. In particular, climate models developed by the same institution have highly correlated biases. Also, somewhat surprisingly, we find evidence that the model skills for simulating the mean climate and simulating the warming trends are not strongly related. © 2008 American Statistical Association.},
   author = {Mikyoung Jun and Reto Knutti and Douglas W. Nychka},
   doi = {10.1198/016214507000001265},
   issn = {01621459},
   issue = {483},
   journal = {Journal of the American Statistical Association},
   keywords = {Cross-covariance model,Intergovernmental Panel on climate change,Kernel smoother,Numerical model evaluation},
   month = {9},
   pages = {934-947},
   title = {Spatial analysis to quantify numerical model bias and dependence: How many climate models are there?},
   volume = {103},
   year = {2008},
}
@article{Katzfuss2011,
   abstract = {The use of satellite measurements in climate studies promises many new scientific insights if those data can be efficiently exploited. Due to sparseness of daily data sets, there is a need to fill spatial gaps and to borrow strength from adjacent days. Nonetheless, these satellites are typically capable of conducting on the order of 100,000 retrievals per day, which makes it impossible to apply traditional spatio-temporal statistical methods, even in supercomputing environments. To overcome these challenges, we make use of a spatio-temporal mixed-effects model. For each massive daily data set, dimension reduction is achieved by essentially modelling the underlying process as a linear combination of spatial basis functions on the globe. The application of a dynamical autoregressive model in time, over the reduced space, allows rapid sequential computation of optimal smoothing predictions via the Kalman smoother; this is known as Fixed Rank Smoothing (FRS). The dimension-reduced mixed-effects model contains a number of unknown parameters, including covariance and propagator matrices, which describe the spatial and temporal dependence structure in the reduced-dimensional process. We take an empirical-Bayes approach to inference, which involves estimating the parameters and substituting them into the optimal predictors. Method-of-moments (MM) parameter estimation (currently used in FRS) is typically inefficient compared to maximum likelihood (ML) estimation and can result in large sampling variability. Here, we develop ML estimation via an expectation-maximization (EM) algorithm, which offers stable computation of valid estimators and makes efficient use of spatial and temporal dependence in the data. The two parameter-estimation approaches, MM and ML, are compared in a simulation study. We also apply our methodology to global satellite CO2 measurements: We optimally smooth the sparse daily CO2 maps obtained by the Atmospheric InfraRed Sounder (AIRS) instrument on the Aqua satellite; then, using FRS with EM-estimated parameters, a complete sequence of the daily global CO2 fields can be obtained, together with their associated prediction uncertainties. © 2011 Blackwell Publishing Ltd.},
   author = {Matthias Katzfuss and Noel Cressie},
   doi = {10.1111/j.1467-9892.2011.00732.x},
   issn = {01439782},
   issue = {4},
   journal = {Journal of Time Series Analysis},
   keywords = {AIRS instrument,EM algorithm,Fixed rank smoothing,Global CO2,Maximum likelihood estimation,Mixed effects models,Spatio-temporal statistics},
   month = {7},
   pages = {430-446},
   title = {Spatio-temporal smoothing and EM estimation for massive remote-sensing data sets},
   volume = {32},
   year = {2011},
}
@article{Katzfuss2012,
   abstract = {Spatio-temporal statistics is prone to the curse of dimensionality: one manifestation of this is inversion of the data-covariance matrix, which is not in general feasible for very-large-to-massive datasets, such as those observed by satellite instruments. This becomes even more of a problem in fully Bayesian statistical models, where the inversion typically has to be carried out many times in Markov chain Monte Carlo samplers. Here, we propose a Bayesian hierarchical spatio-temporal random effects (STRE) model that offers fast computation: Dimension reduction is achieved by projecting the process onto a basis-function space of low, fixed dimension, and the temporal evolution is modeled using a dynamical autoregressive model in time. We develop a multiresolutional prior for the propagator matrix that allows for unknown (random) sparsity and shrinkage, and we describe how sampling from the posterior distribution can be achieved in a feasible way, even if this matrix is very large. Finally, we compare inference based on our fully Bayesian STRE model with that based on an empirical-Bayesian STRE-model approach, where parameters are estimated via an expectation-maximization algorithm. The comparison is carried out in a simulation study and on a real-world dataset of global satellite CO 2 measurements. © 2011 John Wiley & Sons, Ltd.},
   author = {Matthias Katzfuss and Noel Cressie},
   doi = {10.1002/env.1147},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Bayesian hierarchical modelling,Dimension reduction,Global CO 2,Massive datasets,Remote sensing,Varying model dimension},
   month = {2},
   pages = {94-107},
   title = {Bayesian hierarchical spatio-temporal smoothing for very large datasets},
   volume = {23},
   year = {2012},
}
@article{Katzfuss2020,
   abstract = {Many scientific phenomena are studied using computer experiments consisting of multiple runs of a computer model while varying the input settings. Gaussian processes (GPs) are a popular tool for the analysis of computer experiments, enabling interpolation between input settings, but direct GP inference is computationally infeasible for large datasets. We adapt and extend a powerful class of GP methods from spatial statistics to enable the scalable analysis and emulation of large computer experiments. Specifically, we apply Vecchia's ordered conditional approximation in a transformed input space, with each input scaled according to how strongly it relates to the computer-model response. The scaling is learned from the data, by estimating parameters in the GP covariance function using Fisher scoring. Our methods are highly scalable, enabling estimation, joint prediction and simulation in near-linear time in the number of model runs. In several numerical examples, our approach substantially outperformed existing methods.},
   author = {Matthias Katzfuss and Joseph Guinness and Earl Lawrence},
   journal = {arXiv preprint arXiv:2005.00386},
   month = {5},
   title = {Scaled Vecchia approximation for fast computer-model emulation},
   url = {http://arxiv.org/abs/2005.00386},
   year = {2020},
}
@article{Katzfuss2021a,
   abstract = {Gaussian processes (GPs) are commonly used as models for functions, time series, and spatial fields, but they are computationally infeasible for large datasets. Focusing on the typical setting of modeling data as a GP plus an additive noise term, we propose a generalization of the Vecchia (J. Roy. Statist. Soc. Ser. B 50 (1988) 297–312) approach as a framework for GP approximations. We show that our general Vecchia approach contains many popular existing GP approximations as special cases, allowing for comparisons among the different methods within a unified framework. Representing the models by directed acyclic graphs, we determine the sparsity of the matrices necessary for inference, which leads to new insights regarding the computational properties. Based on these results, we propose a novel sparse general Vecchia approximation, which ensures computational feasibility for large spatial datasets but can lead to considerable improvements in approximation accuracy over Vecchia’s original approach. We provide several theoretical results and conduct numerical comparisons. We conclude with guidelines for the use of Vecchia approximations in spatial statistics.},
   author = {Matthias Katzfuss and Joseph Guinness},
   doi = {10.1214/19-STS755},
   issn = {21688745},
   issue = {1},
   journal = {Statistical Science},
   keywords = {Computational complexity,covariance approximation,directed acyclic graphs,large datasets,sparsity,spatial statistics},
   month = {2},
   pages = {124-141},
   publisher = {Institute of Mathematical Statistics},
   title = {A General Framework for Vecchia Approximations of Gaussian Processes},
   volume = {36},
   year = {2021},
}
@article{Katzfuss2021b,
   abstract = {A multivariate distribution can be described by a triangular transport map from the target distribution to a simple reference distribution. We propose Bayesian nonparametric inference on the transport map by modeling its components using Gaussian processes. This enables regularization and and uncertainty quantification of the map estimation, while still resulting in a closed-form and invertible posterior map. We then focus on inferring the distribution of a nonstationary spatial field from a small number of replicates. We develop specific transport-map priors that are highly flexible and are motivated by the behavior of a large class of stochastic processes. Our approach is scalable to high-dimensional fields due to data-dependent sparsity and parallel computations. We also discuss extensions, including Dirichlet process mixtures for flexible marginals. We present numerical results to demonstrate the accuracy, scalability, and usefulness of our methods, including an analysis of non-Gaussian climate-model output.},
   author = {Matthias Katzfuss and Florian Schäfer},
   journal = {arXiv preprint arXiv:2108.04211},
   month = {8},
   title = {Scalable Bayesian transport maps for high-dimensional non-Gaussian spatial fields},
   url = {http://arxiv.org/abs/2108.04211},
   year = {2021},
}
@article{Kaufman2010,
   abstract = {Functional analysis of variance (ANOVA) models partition a functional response according to the main effects and interactions of various factors. This article develops a general framework for functional ANOVA modeling from a Bayesian viewpoint, assigning Gaussian process prior distributions to each batch of functional effects. We discuss the choices to be made in specifying such a model, advocating the treatment of levels within a given factor as dependent but exchangeable quantities, and we suggest weakly informative prior distributions for higher level parameters that may be appropriate in many situations. We discuss computationally efficient strategies for posterior sampling using Markov Chain Monte Carlo algorithms, and we emphasize useful graphical summaries based on the posterior distribution of model-based analogues of traditional ANOVA decompositions of variance. We illustrate this process of model specification, posterior sampling, and graphical posterior summaries in two examples. The first considers the effect of geographic region on the temperature profiles at weather stations in Canada. The second example examines sources of variability in the output of regional climate models from a designed experiment. © 2010 International Society for Bayesian Analysis.},
   author = {Cari G. Kaufman and Stephan R. Sainy},
   doi = {10.1214/10-BA505},
   issn = {19360975},
   issue = {1},
   journal = {Bayesian Analysis},
   keywords = {Analysis of variance,Climate models,Functional data,Variance components},
   pages = {123-150},
   title = {Bayesian functional ANOVA modeling using Gaussian process prior distributions},
   volume = {5},
   year = {2010},
}
@article{Kidd2021,
   abstract = {In spatial statistics, it is often assumed that the spatial field of interest is stationary and its covariance has a simple parametric form, but these assumptions are not appropriate in many applications. Given replicate observations of a Gaussian spatial field, we propose nonstationary and nonparametric Bayesian inference on the spatial dependence. Instead of estimating the quadratic (in the number of spatial locations) entries of the covariance matrix, the idea is to infer a near-linear number of nonzero entries in a sparse Cholesky factor of the precision matrix. Our prior assumptions are motivated by recent results on the exponential decay of the entries of this Cholesky factor for Matern-type covariances under a specific ordering scheme. Our methods are highly scalable and parallelizable. We conduct numerical comparisons and apply our methodology to climate-model output, enabling statistical emulation of an expensive physical model.},
   author = {Brian Kidd and Matthias Katzfuss},
   doi = {10.1214/21-ba1273},
   issn = {19316690},
   issue = {1},
   journal = {Bayesian Analysis},
   month = {6},
   publisher = {Institute of Mathematical Statistics},
   title = {Bayesian Nonstationary and Nonparametric Covariance Estimation for Large Spatial Data (with Discussion)},
   volume = {17},
   year = {2021},
}
@article{Knutti2013,
   abstract = {A new ensemble of climate models is becoming available and provides the basis for climate change projections. Here, we show a first analysis indicating that the models in the new ensemble agree better with observations than those in older ones and that the poorest models have been eliminated. Most models are strongly tied to their predecessors, and some also exchange ideas and code with other models, thus supporting an earlier hypothesis that the models in the new ensemble are neither independent of each other nor independent of the earlier generation. On the basis of one atmosphere model, we show how statistical methods can identify similarities between model versions and complement process understanding in characterizing how and why a model has changed. We argue that the interdependence of models complicates the interpretation of multimodel ensembles but largely goes unnoticed. ©2013 American Geophysical Union. All Rights Reserved.},
   author = {Reto Knutti and David Masson and Andrew Gettelman},
   doi = {10.1002/grl.50256},
   issn = {00948276},
   issue = {6},
   journal = {Geophysical Research Letters},
   keywords = {GCM,independence,model genealogy,multimodel},
   month = {3},
   pages = {1194-1199},
   title = {Climate model genealogy: Generation CMIP5 and how we got there},
   volume = {40},
   year = {2013},
}
@article{Ledoit2004,
    abstract  = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator - the sample covariance matrix - is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample. © 2003 Elsevier Inc. All rights reserved.},
    author    = {Olivier Ledoit and Michael Wolf},
    doi       = {10.1016/S0047-259X(03)00096-4},
    issn      = {0047259X},
    issue     = {2},
    journal   = {Journal of Multivariate Analysis},
    keywords  = {Condition number,Covariance matrix estimation,Empirical Bayes,General asymptotics,Shrinkage},
    pages     = {365-411},
    publisher = {Academic Press Inc.},
    title     = {A well-conditioned estimator for large-dimensional covariance matrices},
    volume    = {88},
    year      = {2004}
}
@article{Marzouk2016,
   abstract = {We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling---i.e., a transport map---between a complex "target" probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available, and second, when the target distribution is known only through a finite collection of samples. We show that in both settings the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization--based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to "Gaussianize" complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization--based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation--based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.},
   author = {Youssef Marzouk and Tarek Moselhy and Matthew Parno and Alessio Spantini},
   doi = {10.1007/978-3-319-11259-6_23-1},
   month = {2},
   title = {An introduction to sampling via measure transport},
   url = {http://arxiv.org/abs/1602.05023 http://dx.doi.org/10.1007/978-3-319-11259-6_23-1},
   year = {2016},
}
@article{Masson2011,
   abstract = {Climate change projections are often given as equally weighted averages across ensembles of climate models, despite the fact that the sampling of the underlying ensembles is unclear. We show that a hierarchical clustering of a metric of spatial and temporal variations of either surface temperature or precipitation in control simulations can capture many model relationships across different ensembles. Strong similarities are seen between models developed at the same institution, between models sharing versions of the same atmospheric component, and between successive versions of the same model. A perturbed parameter ensemble of a model appears separate from other structurally different models. The results provide insight into intermodel relationships, into how models evolve through successive generations, and suggest that assuming model independence in such ensembles of opportunity is not justified. Copyright 2011 by the American Geophysical Union.},
   author = {D. Masson and R. Knutti},
   doi = {10.1029/2011GL046864},
   issn = {00948276},
   issue = {8},
   journal = {Geophysical Research Letters},
   month = {4},
   publisher = {Blackwell Publishing Ltd},
   title = {Climate model genealogy},
   volume = {38},
   year = {2011},
}
@inproceedings{Mcginnis2014,
   abstract = {We evaluate the performance of different distribution mapping techniques for bias-correction of climate model output by operating on synthetic data and comparing the results to an "oracle" correction based on perfect knowledge of the generating distributions. We find results consistent across six different metrics of performance. Techniques based on fitting a distribution perform best on data from normal and gamma distributions, but are at a significant disadvantage when the data does not come from a known parametric distribution. The technique with the best overall performance is a novel technique, Kernel Density Distribution Mapping (KDDM). I. M O T I V A T I O N Climate modeling is a valuable tool for exploring the potential future impacts of climate change whose use is often hindered by bias in the model output. Correcting this bias dramatically increases its usability. In [1], the authors tested a variety of bias correction methods and found that the best overall performer was distribution mapping. Distribution mapping adjusts the individual values of the model data such that their statistical distribution matches that of the observed data. This is accomplished by the method of [2], which constructs a transfer function that transforms model data values to probabilities via the CDF of the model distribution, and then transforms them back into data values using the inverse CDF (or quantile function) of the observational distribution: x corrected = Xfer(x raw) = CDF-1 observed (CDF model (x raw)) There are a number of different techniques for doing distribution mapping, referred to by a variety of different names in the literature, that differ in how they construct the transfer function.We test five different methods: Probability mapping (PMAP) [3:5] fits a parametric distribution to the model and observed datasets using either maximum likelihood estimation or sample moments. The transfer function is the composition of the resulting fitted analytic CDF and quantile functions. Empirical CDF mapping (ECDF) [6,7] sorts the two datasets and maps them against one another, creating the Q-Q map; the transfer function is formed by linear interpolation between the points of the mapping. Quantile mapping (QMAP) [8:10] estimates quantiles for both datasets, then forms a transfer function by interpolation between corresponding quantile values. The number of quantiles is a free parameter; we test small (5), intermediate (N ½), and large (N/5) cases. Asynchronous regional regression modeling (ARRM) [11] is similar to ECDF, but creates its transfer function by fitting the Q-Q map using piecewise linear regression. Kernel density distribution mapping (KDDM) is a novel technique that estimates the PDF of each dataset using kernel density estimation. The PDFs are integrated into CDFs via the trapezoid rule, and the transfer function is formed by mapping the CDFs against one another. II. M E T H O D To evaluate the techniques, we compare them to an ideal correction, or "oracle". We generate three sets of synthetic data to represent observed, modeled current, and modeled future data, using different parameters for each case. The parameter differences between current and future correspond to climate change, and between observational and current datasets to model bias. Knowing the generating distribution and the exact parameter values, we can then construct a perfect transfer function using the appropriate probability and quantile functions. Applying this transfer function to the current dataset makes it statistically indistinguishable from the observed dataset; applying it to the future data generates the "oracle" dataset. We then evaluate each technique by constructing a transfer function in the prescribed way using observed and current data, and applying the result to the future data. The technique's performance is measured in terms of how far it deviates from the perfect correction of the oracle. We perform this procedure using three different distributions, iterating over 1000 realizations of the datasets each time. Each dataset contains 450 data points, which is the size of the datasets we would typically work with when correcting regional climate model output. The three distributions we use are normal, gamma, and a bimodal mixture of two normal distributions. We use the},
   author = {S Mcginnis and Seth Mcginnis and Doug Nychka and Linda O Mearns},
   journal = {4th International Workshop on Climate Informatics},
   title = {A New Distribution Mapping Technique for Climate Model Bias Correction},
   year = {2014},
}
@article{Meinshausen2006,
    abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power. © Institute of Mathematical Statistics, 2006.},
    author   = {Nicolai Meinshausen and Peter Bühlmann},
    doi      = {10.1214/009053606000000281},
    issn     = {00905364},
    issue    = {3},
    journal  = {Annals of Statistics},
    keywords = {Covariance selection,Gaussian graphical models,Linear regression,Penalized regression},
    month    = {6},
    pages    = {1436-1462},
    title    = {High-dimensional graphs and variable selection with the Lasso},
    volume   = {34},
    year     = {2006}
}
@article{Murakami2021,
   abstract = {This study discusses the importance of balancing spatial and non-spatial variation in spatial regression modeling. Unlike spatially varying coefficients (SVC) modeling, which is popular in spatial statistics, non-spatially varying coefficients (NVC) modeling has largely been unexplored in spatial fields. Nevertheless, as we will explain, consideration of non-spatial variation is needed not only to improve model accuracy but also to reduce spurious correlation among varying coefficients, which is a major problem in SVC modeling. We consider a Moran eigenvector approach modeling spatially and non-spatially varying coefficients (S&NVC). A Monte Carlo simulation experiment comparing our S&NVC model with existing SVC models suggests both modeling accuracy and computational efficiency for our approach. Beyond that, somewhat surprisingly, our approach identifies true and spurious correlations among coefficients nearly perfectly, even when usual SVC models suffer from severe spurious correlations. It implies that S&NVC model should be used even when the analysis purpose is modeling SVCs. Finally, our S&NVC model is employed to analyze a residential land price data set. Its results suggest existence of both spatial and non-spatial variation in regression coefficients in practice. The S&NVC model is now implemented in the R package spmoran.},
   author = {Daisuke Murakami and Daniel A. Griffith},
   doi = {10.1111/gean.12310},
   issn = {15384632},
   journal = {Geographical Analysis},
   publisher = {John Wiley and Sons Inc},
   title = {Balancing Spatial and Non-Spatial Variation in Varying Coefficient Modeling: A Remedy for Spurious Correlation},
   year = {2021},
}
@article{Nguyen2014,
   abstract = {Developing global maps of carbon dioxide (CO2) mole fraction (in units of parts per million) near the Earths surface can help identify locations where major amounts of CO2 are entering and exiting the atmosphere, thus providing valuable insights into the carbon cycle and mitigating the greenhouse effect of atmospheric CO2. Existing satellite remote sensing data do not provide measurements of the CO2 mole fraction near the surface. Japans Greenhouse gases Observing SATellite (GOSAT) is sensitive to average CO2 over the entire column, and NASAs Atmospheric InfraRed Sounder (AIRS) is sensitive to CO2 in the middle troposphere. One might expect that lower-atmospheric CO2 could be inferred by differencing GOSAT column-average and AIRS mid-tropospheric data. However, the two instruments have different footprints, measurement-error characteristics, and data coverages. In addition, the spatio-temporal domains are large, and the AIRS dataset is massive. In this article, we describe a spatio-temporal data-fusion (STDF) methodology based on reduced-dimensional Kalman smoothing. Our STDF is able to combine the complementary GOSAT and AIRS datasets to optimally estimate lower-atmospheric CO2 mole fraction over the whole globe. Further, it is designed for massive remote sensing datasets and accounts for differences in instrument footprint, measurement-error characteristics, and data coverages. This article has supplementary material online. © 2014 © 2014 American Statistical Association and the American Society for Quality.},
   author = {Hai Nguyen and Matthias Katzfuss and Noel Cressie and Amy Braverman},
   doi = {10.1080/00401706.2013.831774},
   issn = {15372723},
   issue = {2},
   journal = {Technometrics},
   keywords = {EM algorithm,Fixed rank smoothing,Kalman filter,Multivariate geostatistics,Spatial random effects model},
   month = {4},
   pages = {174-185},
   publisher = {American Statistical Association},
   title = {Spatio-temporal data fusion for very large remote sensing datasets},
   volume = {56},
   year = {2014},
}
@article{Paciorek2006,
   abstract = {We introduce a new class of nonstationary covariance functions for spatial modelling. Nonstationary covariance functions allow the model to adapt to spatial surfaces whose variability changes with location. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the spatial surface is controlled by a parameter, freeing one from fixing the differentiability in advance. The class allows one to knit together local covariance parameters into a valid global nonstationary covariance, regardless of how the local covariance structure is estimated. We employ this new nonstationary covariance in a fully Bayesian model in which the unknown spatial process has a Gaussian process (GP) prior distribution with a nonstationary covariance function from the class. We model the nonstationary structure in a computationally efficient way that creates nearly stationary local behaviour and for which stationarity is a special case. We also suggest non-Bayesian approaches to nonstationary kriging. To assess the method, we use real climate data to compare the Bayesian nonstationary GP model with a Bayesian stationary GP model, various standard spatial smoothing approaches, and nonstationary models that can adapt to function heterogeneity. The GP models outperform the competitors, but while the nonstationary GP gives qualitatively more sensible results, it shows little advantage over the stationary GP on held-out data, illustrating the difficulty in fitting complicated spatial data. Copyright © 2006 John Wiley & Sons, Ltd.},
   author = {Christopher J. Paciorek and Mark J. Schervish},
   doi = {10.1002/env.785},
   issn = {11804009},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Gaussian process,Kernel convolution,Kriging,Smoothing},
   month = {8},
   pages = {483-506},
   title = {Spatial modelling using a new class of nonstationary covariance functions},
   volume = {17},
   year = {2006},
}
@article{Pourahmadi2011,
   abstract = {Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positivedefiniteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions. © Institute of Mathematical Statistics, 2011.},
   author = {Mohsen Pourahmadi},
   doi = {10.1214/11-STS358},
   issn = {08834237},
   issue = {3},
   journal = {Statistical Science},
   keywords = {Bayesian estimation,Cholesky decomposition,Dependence and correlation,Graphical models,Longitudinal data,Parsimony,Penalized likelihood,Precision matrix,Sparsity,Spectral decomposition,Variance-correlation decomposition},
   pages = {369-387},
   title = {Covariance estimation: The GLM and regularization perspectives},
   volume = {26},
   year = {2011},
}
@article{Rothman2008,
   abstract = {The paper proposes a method for constructing a sparse estimator for the inverse covariance (concentration) matrix in high-dimensional settings. The estimator uses a penalized normal likelihood approach and forces sparsity by using a lasso-type penalty. We establish a rate of convergence in the Frobenius norm as both data dimension p and sample size n are allowed to grow, and show that the rate depends explicitly on how sparse the true concentration matrix is. We also show that a correlation-based version of the method exhibits better rates in the operator norm. We also derive a fast iterative algorithm for computing the estimator, which relies on the popular Cholesky decomposition of the inverse but produces a permutation-invariant estimator. The method is compared to other estimators on simulated data and on a real data example of tumor tissue classification using gene expression data. © 2008, Institute of Mathematical Statistics. All rights reserved.},
   author = {Adam J. Rothman and Peter J. Bickel and Elizaveta Levina and Ji Zhu},
   doi = {10.1214/08-EJS176},
   issn = {19357524},
   journal = {Electronic Journal of Statistics},
   keywords = {Cholesky decomposition,Covariance matrix,High dimension low sample size,Large p small n,Lasso,Sparsity},
   pages = {494-515},
   title = {Sparse permutation invariant covariance estimation},
   volume = {2},
   year = {2008},
}
@article{Rothman2012,
   abstract = {Using convex optimization, we construct a sparse estimator of the covariance matrix that is positive definite and performs well in high-dimensional settings. A lasso-type penalty is used to encourage sparsity and a logarithmic barrier function is used to enforce positive definiteness. Consistency and convergence rate bounds are established as both the number of variables and sample size diverge. An efficient computational algorithm is developed and the merits of the approach are illustrated with simulations and a speech signal classification example. © 2012 Biometrika Trust.},
   author = {Adam J. Rothman},
   doi = {10.1093/biomet/ass025},
   issn = {00063444},
   issue = {3},
   journal = {Biometrika},
   keywords = {Barrier function,Classification,Convex optimization,High-dimensional data,Sparsity},
   month = {9},
   pages = {733-740},
   title = {Positive definite estimators of large covariance matrices},
   volume = {99},
   year = {2012},
}
@article{Sampson1992,
  title={Nonparametric estimation of nonstationary spatial covariance structure},
  author={Sampson, Paul D and Guttorp, Peter},
  journal={Journal of the American Statistical Association},
  volume={87},
  number={417},
  pages={108--119},
  year={1992},
  publisher={Taylor \& Francis}
}
@article{Schafer2021a,
   abstract = {Dense kernel matrices Θ ∊ RN× N obtained from point evaluations of a covariance function G at locations \{xi\}1≤i≤N ⊆ Rd arise in statistics, machine learning, and numerical analysis. For covariance functions that are Green's functions of elliptic boundary value problems and homogeneously distributed sampling points, we show how to identify a subset S ⊆ \{1, . . ., N\}2, with #S = O (N log(N) logd(N/∊ )), such that the zero fill-in incomplete Cholesky factorization of the sparse matrix Θ ij1(i,j)∊ S is an ∊-approximation of Θ . This factorization can provably be obtained in complexity O (N log(N) logd(N/∊ )) in space and O (N log2(N) log2d(N/∊ )) in time, improving upon the state of the art for general elliptic operators; we further present numerical evidence that d can be taken to be the intrinsic dimension of the data set rather than that of the ambient space. The algorithm only needs to know the spatial configuration of the xi and does not require an analytic representation of G. Furthermore, this factorization straightforwardly provides an approximate sparse PCA with optimal rate of convergence in the operator norm. Hence, by using only subsampling and the incomplete Cholesky factorization, we obtain, at nearly linear complexity, the compression, inversion, and approximate PCA of a large class of covariance matrices. By inverting the order of the Cholesky factorization we also obtain a solver for elliptic PDE with complexity O (N logd(N/∊ )) in space and O (N log2d(N/∊ )) in time, improving upon the state of the art for general elliptic operators.},
   author = {Florian Schäfer and T. J. Sullivan and Houman Owhadi},
   doi = {10.1137/19M129526X},
   issn = {15403467},
   issue = {2},
   journal = {Multiscale Modeling and Simulation},
   keywords = {Cholesky factorization,Covariance function,Gamblet transform,Kernel matrix,Principal component analysis,Sparsity},
   pages = {688-730},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Compression, inversion, and approximate PCA of dense kernel matrices at near-linear computational complexity},
   volume = {19},
   year = {2021},
}
@article{Schafer2021b,
   abstract = {We propose to compute a sparse approximate inverse Cholesky factor L of a dense covariance matrix Θ by minimizing the Kullback-Leibler divergence between the Gaussian distributions N (0, Θ) and N (0, L -⊤ L - 1), subject to a sparsity constraint. Surprisingly, this problem has a closed-form solution that can be computed efficiently, recovering the popular Vecchia approximation in spatial statistics. Based on recent results on the approximate sparsity of inverse Cholesky factors of Θ obtained from pairwise evaluation of Green's functions of elliptic boundary-value problems at points \{xi\}1≤ i≤ N ⊂ Rd, we propose an elimination ordering and sparsity pattern that allows us to compute ∊-approximate inverse Cholesky factors of such Θ in computational complexity O (N log(N/∊)d) in space and O (N log(N/∊)2d) in time. To the best of our knowledge, this is the best asymptotic complexity for this class of problems. Furthermore, our method is embarrassingly parallel, automatically exploits low-dimensional structure in the data, and can perform Gaussian-process regression in linear (in N) space complexity. Motivated by its optimality properties, we propose applying our method to the joint covariance of training and prediction points in Gaussian-process regression, greatly improving stability and computational cost. Finally, we show how to apply our method to the important setting of Gaussian processes with additive noise, compromising neither accuracy nor computational complexity.},
   author = {Florian Schafer and Matthias Katzfuss and Houman Owhadi},
   doi = {10.1137/20M1336254},
   issn = {10957197},
   issue = {3},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {Cholesky factorization,Factorized approximate inverse,Gaussian process regression,Integral equation,Screening effect,Vecchia approximation},
   pages = {A2019-A2046},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Sparse Cholesky factorization by Kullback-Leibler minimization},
   volume = {43},
   year = {2021},
}
@article{Stein2004,
   abstract = {Likelihood methods are often difficult to use with large, irregularly sited spatial data sets, owing to the computational burden. Even for Gaussian models, exact calculations of the likelihood for n observations require O(n 3) operations. Since any joint density can be written as a product of conditional densities based on some ordering of the observations, one way to lessen the computations is to condition on only some of the 'past' observations when computing the conditional densities. We show how this approach can be adapted to approximate the restricted likelihood and we demonstrate how an estimating equations approach allows us to judge the efficacy of the resulting approximation. Previous work has suggested conditioning on those past observations that are closest to the observation whose conditional density we are approximating. Through theoretical, numerical and practical examples, we show that there can often be considerable benefit in conditioning on some distant observations as well.},
   author = {Michael L. Stein and Zhiyi Chi and Leah J. Welty},
   doi = {10.1046/j.1369-7412.2003.05512.x},
   issn = {13697412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Chlorophyll fluorescence,Estimating equations,Restricted maximum likelihood,Variogram estimation},
   pages = {275-296},
   title = {Approximating likelihoods for large spatial data sets},
   volume = {66},
   year = {2004},
}
@article{Tilmes2018,
   author = {Simone Tilmes and Jadwiga H. Richter and Ben Kravitz and Douglas G. Macmartin and Michael J. Mills and Isla R. Simpson and Anne S. Glanville and John T. Fasullo and Adam S. Phillips and Jean Francois Lamarque and Joseph Tribbia and Jim Edwards and Sheri Mickelson and Siddhartha Ghosh},
   doi = {10.1175/BAMS-D-17-0267.1},
   issn = {00030007},
   issue = {11},
   journal = {Bulletin of the American Meteorological Society},
   month = {11},
   pages = {2361-2371},
   publisher = {American Meteorological Society},
   title = {CESM1(WACCM) stratospheric aerosol geoengineering large ensemble project},
   volume = {99},
   year = {2018},
}
@article{Tingley2012,
   abstract = {Climate datasets with both spatial and temporal components are often studied after removing from each time series a temporal mean calculated over a common reference interval, which is generally shorter than the overall length of the dataset. The use of a short reference interval affects the temporal properties of the variability across the records, by reducing the standard deviation within the reference interval and inflating it elsewhere. For an annually averaged version of the Climate Research Unit's (CRU) temperature anomaly product, the mean standard deviation is 0.67°C within the 1961-90 reference interval, and 0.81°C elsewhere. The calculation of anomalies can be interpreted in terms of a two-factor analysis of variance model. Within a Bayesian inference framework, any missing values are viewed as additional parameters, and the reference interval is specified as the full length of the dataset. This Bayesian scheme is used to re-express the CRU dataset as anomalies with respect to means calculated over the entire 1850-2009 interval spanned by the dataset. The mean standard deviation is increased to 0.69°C within the original 1961-90 reference interval, and reduced to 0.76°C elsewhere. The choice of reference interval thus has a predictable and demonstrable effect on the second spatial moment time series of the CRU dataset. The spatial mean time series is in this case largely unaffected: the amplitude of spatial mean temperature change is reduced by 0.1°C when using the 1850-2009 reference interval, while the 90% uncertainty interval of (-0.03, 0.23) indicates that the reduction is not statistically significant. © 2012 American Meteorological Society.},
   author = {Martin P. Tingley},
   doi = {10.1175/JCLI-D-11-00008.1},
   issn = {08948755},
   issue = {2},
   journal = {Journal of Climate},
   keywords = {Bayesian methods,Climate variability,Statistical techniques,Temperature},
   month = {1},
   pages = {777-791},
   title = {A bayesian ANOVA scheme for calculating climate anomalies, with applications to the instrumental temperature record},
   volume = {25},
   year = {2012},
}
@article{Vecchia1988,
   abstract = {Formal parameter estimation and model identification procedures for continuous domain spatial processes are introduced. The processes are assumed to be adequately described by a linear model with residuals that follow a second-order stationary Gaussian random field and data are assumed to consist of noisy observations of the process at arbitrary sampling locations. A general class of two-dimensional rational spectral density functions with elliptic contours is used to model the spatial covariance function. An iterative estimation procedure alleviates many of the computational difficulties of conventional maximum likelihood estimation for non-lattice data. The procedure is applied to several generated data sets and to an actual ground-water data set.},
   author = {A. V. Vecchia},
   doi = {10.1111/j.2517-6161.1988.tb01729.x},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   month = {1},
   pages = {297-312},
   publisher = {Wiley},
   title = {Estimation and Model Identification for Continuous Spatial Processes},
   volume = {50},
   year = {1988},
}
@article{Wheeler2007,
   abstract = {The realization in the statistical and geographical sciences that a relationship between an explanatory variable and a response variable in a linear regression model is not always constant across a study area has led to the development of regression models that allow for spatially varying coefficients. Two competing models of this type are geographically weighted regression (GWR) and Bayesian regression models with spatially varying coefficient processes (SVCP). In the application of these spatially varying coefficient models, marginal inference on the regression coefficient spatial processes is typically of primary interest. In light of this fact, there is a need to assess the validity of such marginal inferences, since these inferences may be misleading in the presence of explanatory variable collinearity. In this paper, we present the results of a simulation study designed to evaluate the sensitivity of the spatially varying coefficients in the competing models to various levels of collinearity. The simulation study results show that the Bayesian regression model produces more accurate inferences on the regression coefficients than does GWR. In addition, the Bayesian regression model is overall fairly robust in terms of marginal coefficient inference to moderate levels of collinearity, and degrades less substantially than GWR with strong collinearity. © Springer-Verlag 2007.},
   author = {David C. Wheeler and Catherine A. Calder},
   doi = {10.1007/s10109-006-0040-y},
   issn = {14355949},
   issue = {2},
   journal = {Journal of Geographical Systems},
   keywords = {Bayesian regression,Collinearity,Geographically weighted regression,MCMC,Simulation study,Spatial statistics},
   pages = {145-166},
   publisher = {Springer Verlag},
   title = {An assessment of coefficient accuracy in linear regression models with spatially varying coefficients},
   volume = {9},
   year = {2007},
}
